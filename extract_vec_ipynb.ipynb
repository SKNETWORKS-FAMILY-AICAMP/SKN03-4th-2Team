{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in .\\.venv\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in .\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in .\\.venv\\lib\\site-packages (from langchain) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in .\\.venv\\lib\\site-packages (from langchain) (3.10.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in .\\.venv\\lib\\site-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in .\\.venv\\lib\\site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in .\\.venv\\lib\\site-packages (from langchain) (0.1.139)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in .\\.venv\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in .\\.venv\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in .\\.venv\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in .\\.venv\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in .\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in .\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in .\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in .\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in .\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in .\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in .\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in .\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in .\\.venv\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in .\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in .\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in .\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in .\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in .\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in .\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in .\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in .\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
      "Requirement already satisfied: sniffio in .\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in .\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in .\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in .\\.venv\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: pandas in .\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in .\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:dotenv.main:Python-dotenv could not parse statement starting at line 11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import NotebookLoader\n",
    "from langchain.document_loaders import CSVLoader\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Chunk 0]\n",
      "\n",
      "Content:\n",
      "역번호                                          0124\n",
      "역사명                                  청량리(서울시립대입구)\n",
      "노선명                                           1호선\n",
      "환승역구분                                    도시철도 환승역\n",
      "환승노선명    수도권  광역철도 분당+수도권  광역철도 경춘+수도권  광역철도 경의중앙\n",
      "\n",
      "Metadata:\n",
      "{'source_file': 'subwaystation.csv', 'source_folder': 'mini'}\n",
      "============================================================\n",
      "[Chunk 1]\n",
      "\n",
      "Content:\n",
      "역번호          0125\n",
      "역사명           제기동\n",
      "노선명           1호선\n",
      "환승역구분    도시철도 일반역\n",
      "환승노선명         NaN\n",
      "\n",
      "Metadata:\n",
      "{'source_file': 'subwaystation.csv', 'source_folder': 'mini'}\n",
      "============================================================\n",
      "[Chunk 2]\n",
      "\n",
      "Content:\n",
      "역번호               0126\n",
      "역사명                신설동\n",
      "노선명                1호선\n",
      "환승역구분         도시철도 환승역\n",
      "환승노선명    수도권  도시철도 2호선\n",
      "\n",
      "Metadata:\n",
      "{'source_file': 'subwaystation.csv', 'source_folder': 'mini'}\n",
      "============================================================\n",
      "[Chunk 3]\n",
      "\n",
      "Content:\n",
      "역번호               0127\n",
      "역사명                동묘앞\n",
      "노선명                1호선\n",
      "환승역구분         도시철도 환승역\n",
      "환승노선명    수도권  도시철도 6호선\n",
      "\n",
      "Metadata:\n",
      "{'source_file': 'subwaystation.csv', 'source_folder': 'mini'}\n",
      "============================================================\n",
      "[Chunk 4]\n",
      "\n",
      "Content:\n",
      "역번호               0128\n",
      "역사명                동대문\n",
      "노선명                1호선\n",
      "환승역구분         도시철도 환승역\n",
      "환승노선명    수도권  광역철도 4호선\n",
      "\n",
      "Metadata:\n",
      "{'source_file': 'subwaystation.csv', 'source_folder': 'mini'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# 파일 경로 설정\n",
    "notebook_path = './extract_vec_ipynb.ipynb'\n",
    "csv_path = './subwaystation.csv'\n",
    "\n",
    "# .csv 파일을 불러오기\n",
    "import pandas as pd\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(csv_path, encoding='utf-8', encoding_errors='replace')  # 인코딩 문제를 피하기 위해 'errors' 인자를 사용\n",
    "\n",
    "# CSV 파일의 각 행을 Document 형식으로 변환\n",
    "documents = [\n",
    "    Document(page_content=row.to_string(), metadata={'source_file': 'subwaystation.csv', 'source_folder': 'mini'})\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# 텍스트 분할 설정\n",
    "text_splitter = MarkdownTextSplitter(chunk_size=600, chunk_overlap=200)\n",
    "\n",
    "# 분할된 문서 리스트 생성\n",
    "docs = []\n",
    "for doc in documents:\n",
    "    split_docs = text_splitter.create_documents([doc.page_content])\n",
    "    for split_doc in split_docs:\n",
    "        split_doc.metadata = doc.metadata\n",
    "    docs.extend(split_docs)\n",
    "\n",
    "# 분할된 문서 예시 출력\n",
    "for i, doc in enumerate(docs[:5]):\n",
    "    print(f\"[Chunk {i}]\\n\")\n",
    "    print(\"Content:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"\\nMetadata:\")\n",
    "    print(doc.metadata) \n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initializing embedding function\n",
      "INFO:root:Adding documents to FAISS vector store\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Documents added to FAISS vector store\n",
      "INFO:root:FAISS vector store saved locally to './langagent/faiss'\n",
      "INFO:root:Metadata saved locally to './langagent/faiss/metadata.json'\n"
     ]
    }
   ],
   "source": [
    "# Step 4: 임베딩 함수 초기화 및 FAISS 벡터 스토어 저장\n",
    "try:\n",
    "    logging.info(\"Initializing embedding function\")\n",
    "    embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    logging.info(\"Adding documents to FAISS vector store\")\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embedding_function\n",
    "    )\n",
    "    logging.info(\"Documents added to FAISS vector store\")\n",
    "\n",
    "    # 로컬 디렉터리에 FAISS 벡터 스토어 저장\n",
    "    vectorstore.save_local('./langagent/faiss')\n",
    "    logging.info(\"FAISS vector store saved locally to './langagent/faiss'\")\n",
    "\n",
    "    # 메타데이터 저장\n",
    "    metadata = [doc.metadata for doc in docs]  # 모든 문서의 메타데이터 수집\n",
    "    with open('./langagent/faiss/metadata.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(metadata, f, ensure_ascii=False, indent=4)\n",
    "    logging.info(\"Metadata saved locally to './langagent/faiss/metadata.json'\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error adding documents to FAISS vector store: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:dotenv.main:Python-dotenv could not parse statement starting at line 11\n",
      "INFO:root:FAISS vector store loaded from './langagent/faiss'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import logging\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = FAISS.load_local('./langagent/faiss', embedding_function, allow_dangerous_deserialization=True)\n",
    "logging.info(\"FAISS vector store loaded from './langagent/faiss'\")\n",
    "\n",
    "def query_restaurant_recommendations(user_input):\n",
    "    try:\n",
    "        logging.info(f\"Starting query search: {user_input}\")\n",
    "\n",
    "        # Use the vectorstore's as_retriever method\n",
    "        retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "        search_results = retriever.get_relevant_documents(user_input)\n",
    "\n",
    "        logging.info(\"Query search completed\")\n",
    "\n",
    "        cleaned_results = []\n",
    "        for doc in search_results:\n",
    "            content = doc.page_content.strip()\n",
    "            metadata = doc.metadata \n",
    "            cleaned_results.append({\n",
    "                \"content\": content,\n",
    "                \"metadata\": metadata \n",
    "            })\n",
    "        return cleaned_results\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during query search: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting query search: 혜화역에서 가산디지털단지역까지 어떻게 가?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Query search completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'content': '역번호               0746\\n역사명            가산디지털단지\\n노선명                7호선\\n환승역구분         도시철도 환승역\\n환승노선명    수도권  광역철도 1호선',\n",
       "  'metadata': {'source_file': 'subwaystation.csv', 'source_folder': 'mini'}},\n",
       " {'content': '역번호          1702\\n역사명      가산디지털단지역\\n노선명           경부선\\n환승역구분         환승역\\n환승노선명         7호선',\n",
       "  'metadata': {'source_file': 'subwaystation.csv', 'source_folder': 'mini'}},\n",
       " {'content': '역번호           1266\\n역사명      디지털미디어시티역\\n노선명          경의중앙선\\n환승역구분          환승역\\n환승노선명      6호선, 공항',\n",
       "  'metadata': {'source_file': 'subwaystation.csv', 'source_folder': 'mini'}},\n",
       " {'content': '역번호                    A04\\n역사명               디지털미디어시티\\n노선명                인천국제공항선\\n환승역구분                  환승역\\n환승노선명    서울 도시철도 6호선+경의중앙선',\n",
       "  'metadata': {'source_file': 'subwaystation.csv', 'source_folder': 'mini'}}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_restaurant_recommendations('혜화역에서 가산디지털단지역까지 어떻게 가?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
